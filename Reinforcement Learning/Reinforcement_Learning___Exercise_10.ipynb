{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tutsilianna/Introduction_to_ML_and_Advanced_ML_Python/blob/main/Reinforcement%20Learning/Reinforcement_Learning_%7C_Exercise_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9QLe_T6GZUd"
      },
      "source": [
        "# Programming task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYlIf2yHv8hz"
      },
      "source": [
        "**The task should be completed with the current values of the hyperparameters. For verification, below you will see the answers that should be obtained. After all the answers match, you can use the resulting notebook to complete your individual task.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDQzNIZXAoFE"
      },
      "source": [
        "Set model hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOMw2ZbOAmOZ"
      },
      "source": [
        "epsilon = 0.1   # Epsilon parameter which is used in epsilon-greedy strategy\n",
        "gamma = 0.8     # Discount coefficient gamma\n",
        "random_seed = 6 # Random seed\n",
        "time_delay = 1  # Time delay when rendering the game process after training (seconds)\n",
        "lr_rate = 0.9   # Learning rate alpha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQu5IYHX8jId"
      },
      "source": [
        "We import the libraries, create our own 6x6 environment. S denotes the starting point. F - ice is safe (frozen), H - hole, G - goal. The parameter `is_slippery = False` is responsible for slipping. That is, if the agent chose the action to go right, then it will move to the corresponding state. In the general case, due to the “slipping”, one may find itself in a different state. We also copied from the GYM library and slightly modified the function `` generate_random_map ``, in order to generate arbitrary maps based on `` random_seed ``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awL7CCCwD6C3",
        "outputId": "22b5b4df-1cce-45ca-c72b-6f97360d73aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/dvolchek/gym_0_18_0.git -q\n",
        "%cd /content/gym_0_18_0\n",
        "!pip install -e. -q\n",
        "\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "def generate_random_map(size, p, sd):\n",
        "    \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
        "    :param size: size of each side of the grid\n",
        "    :param p: probability that a tile is frozen\n",
        "    \"\"\"\n",
        "    valid = False\n",
        "    np.random.seed(sd)\n",
        "\n",
        "    # DFS to check that it's a valid path.\n",
        "    def is_valid(res):\n",
        "        frontier, discovered = [], set()\n",
        "        frontier.append((0,0))\n",
        "        while frontier:\n",
        "            r, c = frontier.pop()\n",
        "            if not (r,c) in discovered:\n",
        "                discovered.add((r,c))\n",
        "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
        "                for x, y in directions:\n",
        "                    r_new = r + x\n",
        "                    c_new = c + y\n",
        "                    if r_new < 0 or r_new >= size or c_new < 0 or c_new >= size:\n",
        "                        continue\n",
        "                    if res[r_new][c_new] == 'G':\n",
        "                        return True\n",
        "                    if (res[r_new][c_new] not in '#H'):\n",
        "                        frontier.append((r_new, c_new))\n",
        "        return False\n",
        "\n",
        "    while not valid:\n",
        "        p = min(1, p)\n",
        "        res = np.random.choice(['F', 'H'], (size, size), p=[p, 1-p])\n",
        "        res[0][0] = 'S'\n",
        "        res[-1][-1] = 'G'\n",
        "        valid = is_valid(res)\n",
        "    return [\"\".join(x) for x in res]\n",
        "\n",
        "# Map generation\n",
        "random_map = generate_random_map(size=6, p=0.8, sd = random_seed) #Create our map\n",
        "env = gym.make(\"FrozenLake-v0\", desc=random_map, is_slippery=False) #Initialize environment\n",
        "print(\"Your map\")\n",
        "env.render() #Render the map"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gym_0_18_0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Your map\n",
            "\n",
            "\u001b[41mS\u001b[0mFHFFF\n",
            "FFFFFF\n",
            "FFFHHF\n",
            "HHFFHF\n",
            "FFFHFF\n",
            "FHFFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDCexoEU9a_c"
      },
      "source": [
        "Functions for selecting an action and updating an action value table. The line *** is used to check responses in openedx. Outside of the academic problem, it is better to use the original method of the `environment` class, that is:\n",
        "\n",
        "`action = env.action_space.sample ()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5TbDqn6G_Pt"
      },
      "source": [
        "# Task 1\n",
        "Complete the function `` learn () ``, so that as a result of its call, the value of the current action is updated according to the Q-learning algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdQBpxaTOK7u"
      },
      "source": [
        "def choose_action(state):\n",
        "    action=0\n",
        "    if np.random.uniform(0, 1) < epsilon:\n",
        "        action = np.random.randint(0,env.action_space.n) #***\n",
        "    else:\n",
        "        action = np.random.choice(np.array(np.argwhere(Q[state, :] == np.amax(Q[state, :])).flatten().tolist()))\n",
        "    return action\n",
        "\n",
        "def learn(state, state2, reward, action, done):\n",
        "    if done:\n",
        "        Q[state, action] = Q[state, action] + lr_rate * (reward - Q[state, action]) # терминальное состояние\n",
        "    else:\n",
        "        Q[state, action] = Q[state, action] + lr_rate * (reward + gamma * np.max(Q[state2,:]) - Q[state, action])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7COGeyA_Ist3"
      },
      "source": [
        "# Task 2\n",
        "Complete the following code so that as a result of training the model you could find out the number of wins and the number of the game (`game`), on which the agent won the fifth victory in a row for the first time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0adDl7NvJoQP"
      },
      "source": [
        "Let's explain what the function ```env.step(action)``` returns\n",
        "\n",
        "```state2``` --  next state\n",
        "\n",
        "```reward``` -- reward\n",
        "\n",
        "```done``` -- inidcator of the end of the game. True in case of victory or fall into the hole. False in other cases.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq92-dWiOchF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "463734bc-87c5-488a-cd34-477fa78f7984"
      },
      "source": [
        "from tqdm import tqdm\n",
        "# Inititalization\n",
        "np.random.seed(random_seed)\n",
        "wins = []\n",
        "total_games = 10000\n",
        "max_steps = 100\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "episode = 0\n",
        "#Main cycle\n",
        "for game in tqdm(range(total_games)):\n",
        "    state = env.reset()\n",
        "    t = 0\n",
        "    while t < max_steps:\n",
        "        if game > 5 and wins[game-5] == 1 and wins[game-4] == 1 and wins[game-3] == 1 and wins[game-2] == 1 and wins[game-1] == 1 and episode == 0:\n",
        "          episode = game\n",
        "        t += 1\n",
        "\n",
        "        action = choose_action(state)\n",
        "\n",
        "        state2, reward, done, info = env.step(action)\n",
        "\n",
        "        if t == max_steps:\n",
        "            done = True\n",
        "\n",
        "        learn(state, state2, reward, action, done)\n",
        "\n",
        "        state = state2\n",
        "\n",
        "        if done and reward == 1:\n",
        "            wins.append(1)\n",
        "            break\n",
        "        if done:\n",
        "            wins.append(0)\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:10<00:00, 968.46it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFuxsqdRLOS9"
      },
      "source": [
        "Output answers with the given parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZbJtFnhLa7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f3ef4e3-1a57-4db6-c21d-b769f3f5651d"
      },
      "source": [
        "print(\"The number of victories in a series of 10,000 games: \", np.sum(wins))\n",
        "print(\"Five wins in a row were first won in the game \", episode)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of victories in a series of 10,000 games:  2202\n",
            "Five wins in a row were first won in the game  7473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSXdSiG2WI71"
      },
      "source": [
        "The following results should be obtained.\n",
        "\n",
        "\n",
        "*  The number of victories in a series of 10,000 games:  7914\n",
        "*  Five wins in a row were first won in the game  885\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nazZaAbwQGBt"
      },
      "source": [
        "Let's perform one game to track the actions of the agent. At the same time, we will consider the model fully trained, that is, actions are selected according to the greedy strategy, the values of the actions in the table are not updated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ysllZjEQXLa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38f6972b-d700-4513-ff61-8880a9902b1d"
      },
      "source": [
        "import time\n",
        "#Greedy action selection\n",
        "def choose_action_one_game(state):\n",
        "    action = np.random.choice(np.array(np.argwhere(Q[state, :] == np.amax(Q[state, :])).flatten().tolist()))\n",
        "    return action\n",
        "\n",
        "states=[]#Array to save agent states during the game\n",
        "t = 0\n",
        "state = env.reset()\n",
        "wn = 0\n",
        "while(t<100):\n",
        "  env.render()\n",
        "  time.sleep(time_delay)\n",
        "  clear_output(wait=True)\n",
        "  action = choose_action_one_game(state)\n",
        "  state2, reward, done, info = env.step(action)\n",
        "  states.append(state)\n",
        "  state = state2\n",
        "  t += 1\n",
        "  if done and reward == 1:\n",
        "    wn=1\n",
        "  if done:\n",
        "    break\n",
        "if wn == 1:\n",
        "  print(\"!!!WIN!!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!!WIN!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x696NulpReFI"
      },
      "source": [
        "Route map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKMCMdpOTcXy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "aec28525-9753-4ecf-d355-48a78dc3e96d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def make_maze_pic(maze):\n",
        "  maze_pic=[]\n",
        "  for i in range(len(maze)):\n",
        "    row = []\n",
        "    for j in range(len(maze[i])):\n",
        "      if maze[i][j] == 'S':\n",
        "        row.append(0)\n",
        "      if maze[i][j] == 'F':\n",
        "        row.append(0)\n",
        "      if maze[i][j] == 'H':\n",
        "        row.append(1)\n",
        "      if maze[i][j] == 'G':\n",
        "        row.append(0)\n",
        "    maze_pic.append(row)\n",
        "  maze_pic = np.array(maze_pic)\n",
        "  return maze_pic\n",
        "\n",
        "\n",
        "#Make maze fit to plot\n",
        "maze_pic = make_maze_pic(random_map)\n",
        "nrows, ncols = maze_pic.shape\n",
        "\n",
        "#Arrays of picture elements\n",
        "rw = np.remainder(states,nrows)\n",
        "cl = np.floor_divide(states,nrows)\n",
        "if wn == 1:\n",
        "  rw = np.append(rw, [nrows-1])\n",
        "  cl = np.append(cl,[ncols-1])\n",
        "\n",
        "#Picture plotting\n",
        "fig, ax1 = plt.subplots(1, 1, tight_layout=True)\n",
        "ax1.clear()\n",
        "ax1.set_xticks(np.arange(0.5, nrows, step=1))\n",
        "ax1.set_xticklabels([])\n",
        "ax1.set_yticks(np.arange(0.5, ncols, step=1))\n",
        "ax1.set_yticklabels([])\n",
        "ax1.grid(True)\n",
        "ax1.plot([0],[0], \"gs\", markersize=40)  # start is a big green square\n",
        "ax1.text(0, 0.2,\"Start\", ha=\"center\", va=\"center\", color=\"white\", fontsize=12) #Start text\n",
        "ax1.plot([nrows-1],[ncols-1], \"rs\", markersize=40)  # exit is a big red square\n",
        "ax1.text(nrows-1, ncols-1+0.2,\"Finish\", ha=\"center\", va=\"center\", color=\"white\", fontsize=12) #Exit text\n",
        "ax1.plot(rw,cl, ls = '-', color = 'blue') #Blue lines path\n",
        "ax1.plot(rw,cl, \"bo\")  # Blue dots visited cells\n",
        "ax1.imshow(maze_pic, cmap=\"binary\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ae67eadd960>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}